{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgVgrM3LIs4VsPFEejpQyz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qorah/vic-edu-housing-insights/blob/main/vce_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4-V9HMbK6_E",
        "outputId": "a174011a-de23-4967-c14d-34f35b3c80c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies imported and directories created.\n"
          ]
        }
      ],
      "source": [
        "#@title Cell 1: Import Dependencies, Configure Logging & Create Directories\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def configure_logging():\n",
        "    \"\"\"\n",
        "    Configures the logging settings for the pipeline.\n",
        "\n",
        "    Logging is set to INFO level with a format that includes timestamp, log level, and message.\n",
        "    \"\"\"\n",
        "    logging.basicConfig(level=logging.INFO,\n",
        "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "    logging.info(\"Logging is configured.\")\n",
        "\n",
        "def create_directories():\n",
        "    \"\"\"\n",
        "    Creates necessary directories for raw data, processed data, and visualizations.\n",
        "\n",
        "    Ensures that the directories exist; if not, they will be created.\n",
        "    \"\"\"\n",
        "    directories = ['data/raw', 'data/processed', 'visualizations']\n",
        "    for dir_path in directories:\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "        logging.info(f\"Directory created or already exists: {dir_path}\")\n",
        "\n",
        "# Execute setup functions\n",
        "configure_logging()\n",
        "create_directories()\n",
        "\n",
        "logging.info(\"Dependencies imported and directories created successfully.\")\n",
        "print(\"Dependencies imported and directories created.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 2: Data Exploration and Read Data\n",
        "def explore_excel_structure(file_path):\n",
        "    \"\"\"\n",
        "    Explores the structure of an Excel file by trying different header rows.\n",
        "\n",
        "    Iterates through potential header rows and logs the sheet names, data shape, and first few columns.\n",
        "    Returns the header row that appears to structure the data correctly along with a sample of the data.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): The complete path to the Excel file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (header_row (int), sample_data (pd.DataFrame)) if a valid header is found;\n",
        "               (None, None) otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        xls = pd.ExcelFile(file_path)\n",
        "        sheet_names = xls.sheet_names\n",
        "        logging.info(f\"Available sheet names: {sheet_names}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to open Excel file at {file_path}: {e}\")\n",
        "        raise e\n",
        "\n",
        "    # Attempt to identify the correct header row based on data shape and column count\n",
        "    for header_row in range(0, 15):\n",
        "        try:\n",
        "            df = pd.read_excel(file_path, sheet_name=sheet_names[0], header=header_row)\n",
        "            logging.info(f\"Attempt with header_row={header_row}, DataFrame shape: {df.shape}\")\n",
        "            logging.info(f\"First few columns: {list(df.columns)[:5]}\")\n",
        "            # Example condition: valid header if more than 10 columns are detected\n",
        "            if df.shape[1] > 10:\n",
        "                return header_row, df.head(2)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error reading with header_row={header_row}: {e}\")\n",
        "    logging.error(\"Unable to determine a valid header row for the Excel file.\")\n",
        "    return None, None\n",
        "\n",
        "def read_data(file_name, sheet_name=None, header_row=None):\n",
        "    \"\"\"\n",
        "    Reads data from an Excel file stored in the 'data/raw' directory.\n",
        "\n",
        "    If header_row is not provided, the function will call explore_excel_structure to determine it.\n",
        "    If sheet_name is not provided, the first sheet is used by default.\n",
        "\n",
        "    Parameters:\n",
        "        file_name (str): Name of the Excel file (must be located in 'data/raw').\n",
        "        sheet_name (str, optional): The sheet name to read. Defaults to the first sheet.\n",
        "        header_row (int, optional): The row number to use as header. If None, auto-detection is attempted.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The loaded DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a valid header row cannot be determined.\n",
        "    \"\"\"\n",
        "    file_path = os.path.join('data/raw', file_name)\n",
        "    logging.info(f\"Attempting to read file: {file_path}\")\n",
        "\n",
        "    # Determine the header row if not provided\n",
        "    if header_row is None:\n",
        "        header_row, sample = explore_excel_structure(file_path)\n",
        "        if header_row is None:\n",
        "            error_msg = \"Could not determine an appropriate header row from the Excel file.\"\n",
        "            logging.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "        else:\n",
        "            logging.info(f\"Detected header row {header_row}. Sample data:\\n{sample}\")\n",
        "\n",
        "    # Use the first sheet if sheet_name is not provided\n",
        "    if sheet_name is None:\n",
        "        try:\n",
        "            sheet_name = pd.ExcelFile(file_path).sheet_names[0]\n",
        "            logging.info(f\"No sheet name provided; defaulting to first sheet: {sheet_name}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error retrieving sheet names: {e}\")\n",
        "            raise e\n",
        "\n",
        "    try:\n",
        "        df = pd.read_excel(file_path, sheet_name=sheet_name, header=header_row)\n",
        "        logging.info(f\"Successfully loaded {file_name} with header_row={header_row}. DataFrame shape: {df.shape}\")\n",
        "        logging.info(f\"Column names: {list(df.columns)}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading data from {file_name}: {e}\")\n",
        "        raise e\n",
        "\n",
        "# For testing in Colab, adjust the file name as needed.\n",
        "file_name = \"2024SeniorSecondaryCompletionAndAchievementInformation.xlsx\"\n",
        "header, sample = explore_excel_structure(os.path.join('data/raw', file_name))\n",
        "print(\"Detected header row:\", header)\n",
        "print(\"Sample data:\\n\", sample)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XnPObF4K_pM",
        "outputId": "a50b7004-63f0-46cd-c62e-939fd66a12d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected header row: 0\n",
            "Sample data:\n",
            "   Senior Secondary Completion and Achievement Information, 2024 Unnamed: 1  \\\n",
            "0                                                NaN                   NaN   \n",
            "1                                              Notes                   NaN   \n",
            "\n",
            "  Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7  \\\n",
            "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
            "\n",
            "  Unnamed: 8 Unnamed: 9  ... Unnamed: 13 Unnamed: 14 Unnamed: 15 Unnamed: 16  \\\n",
            "0        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
            "1        NaN        NaN  ...         NaN         NaN         NaN         NaN   \n",
            "\n",
            "  Unnamed: 17 Unnamed: 18 Unnamed: 19 Unnamed: 20 Unnamed: 21 Unnamed: 22  \n",
            "0         NaN         NaN         NaN         NaN         NaN         NaN  \n",
            "1         NaN         NaN         NaN         NaN         NaN         NaN  \n",
            "\n",
            "[2 rows x 23 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 3: Data Cleaning\n",
        "def clean_data(df):\n",
        "    \"\"\"\n",
        "    Clean the DataFrame by handling special values and converting data types.\n",
        "\n",
        "    Steps:\n",
        "      - Display unique values in columns (if few) for understanding.\n",
        "      - Use the first column as the school name column.\n",
        "      - Identify and process a \"Small School\" column if available.\n",
        "      - Replace special characters/codes (e.g., '-', '<4', 'I/D') with 0.\n",
        "      - Convert columns to numeric types when applicable.\n",
        "      - Identify key columns such as VCE median score and high scores percentage.\n",
        "      - Mark selective schools by updating the 'Sector' column.\n",
        "\n",
        "    Returns:\n",
        "      cleaned_df: The cleaned DataFrame.\n",
        "      school_col: The column used as the school name.\n",
        "      vce_median_col: The column representing the VCE median score.\n",
        "      high_scores_col: The column representing high scores percentage (if found).\n",
        "    \"\"\"\n",
        "    cleaned_df = df.copy()\n",
        "\n",
        "    # Display unique values in each column if there are few unique values\n",
        "    for col in cleaned_df.columns:\n",
        "        unique_vals = cleaned_df[col].unique()\n",
        "        if len(unique_vals) < 10:\n",
        "            logging.info(f\"Unique values in {col}: {unique_vals}\")\n",
        "\n",
        "    # Use the first column as the school name column\n",
        "    school_col = cleaned_df.columns[0]\n",
        "    logging.info(f\"Using {school_col} as school name column\")\n",
        "\n",
        "    # Check for a \"Small School\" column (might be named differently)\n",
        "    small_school_col = None\n",
        "    for col in cleaned_df.columns:\n",
        "        if isinstance(col, str) and 'small' in col.lower():\n",
        "            small_school_col = col\n",
        "            break\n",
        "        elif col == 'Unnamed: 1':  # Possibility based on file structure\n",
        "            if '*' in cleaned_df[col].values:\n",
        "                small_school_col = col\n",
        "                break\n",
        "\n",
        "    if small_school_col:\n",
        "        logging.info(f\"Found Small School column: {small_school_col}\")\n",
        "        # Remove header rows that contain column descriptions\n",
        "        cleaned_df = cleaned_df[~cleaned_df[small_school_col].astype(str).str.contains('Small School', na=False)]\n",
        "        # Replace '*' with 1 and convert to integer\n",
        "        cleaned_df[small_school_col] = cleaned_df[small_school_col].replace('*', 1)\n",
        "        cleaned_df[small_school_col] = cleaned_df[small_school_col].fillna(0).astype(int)\n",
        "        # Drop small schools if indicated\n",
        "        cleaned_df = cleaned_df[cleaned_df[small_school_col] == 0]\n",
        "    else:\n",
        "        logging.warning(\"Small School column not found\")\n",
        "\n",
        "    # Replace '-' with 0\n",
        "    cleaned_df = cleaned_df.replace('-', 0)\n",
        "\n",
        "    # Replace patterns like '-.1', '-.2', etc. with 0\n",
        "    for col in cleaned_df.columns:\n",
        "        cleaned_df[col] = cleaned_df[col].astype(str).replace(r'-\\.\\d+', '0', regex=True)\n",
        "\n",
        "    # Replace 'I/D' (indicating no enrollment) with 0\n",
        "    cleaned_df = cleaned_df.replace('I/D', 0)\n",
        "\n",
        "    # Replace '<4' and '< 4' with 0\n",
        "    cleaned_df = cleaned_df.replace('<4', 0)\n",
        "    cleaned_df = cleaned_df.replace('< 4', 0)\n",
        "\n",
        "    # Check for an IB column and process it\n",
        "    ib_col = None\n",
        "    for col in cleaned_df.columns:\n",
        "        if isinstance(col, str) and ('ib' in col.lower() or 'international' in col.lower()):\n",
        "            ib_col = col\n",
        "            break\n",
        "        elif col == 'Unnamed: 8':\n",
        "            if 'Y' in cleaned_df[col].values:\n",
        "                ib_col = col\n",
        "                break\n",
        "    if ib_col:\n",
        "        logging.info(f\"Found IB column: {ib_col}\")\n",
        "        cleaned_df[ib_col] = cleaned_df[ib_col].replace('Y', 1).fillna(0)\n",
        "\n",
        "    # Check for an Adult column and filter out adult sectors\n",
        "    adult_col = None\n",
        "    for col in cleaned_df.columns:\n",
        "        if isinstance(col, str) and 'adult' in col.lower():\n",
        "            adult_col = col\n",
        "            break\n",
        "    if adult_col:\n",
        "        logging.info(f\"Found Adult column: {adult_col}\")\n",
        "        cleaned_df[adult_col] = cleaned_df[adult_col].replace('A', 1).fillna(0)\n",
        "        cleaned_df = cleaned_df[cleaned_df[adult_col] == 0]\n",
        "\n",
        "    # Add a 'Sector' column; default value is 'REGULAR'\n",
        "    cleaned_df['Sector'] = 'REGULAR'\n",
        "\n",
        "    # Convert columns to numeric where applicable\n",
        "    numeric_cols = []\n",
        "    for col in cleaned_df.columns:\n",
        "        if col in [school_col, 'Sector'] or (isinstance(col, str) and 'local' in col.lower()):\n",
        "            continue\n",
        "        try:\n",
        "            pd.to_numeric(cleaned_df[col], errors='raise')\n",
        "            numeric_cols.append(col)\n",
        "        except:\n",
        "            numeric_count = pd.to_numeric(cleaned_df[col], errors='coerce').notna().sum()\n",
        "            if numeric_count > len(cleaned_df) * 0.5:\n",
        "                numeric_cols.append(col)\n",
        "\n",
        "    logging.info(f\"Identified numeric columns: {numeric_cols}\")\n",
        "    for col in numeric_cols:\n",
        "        cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    # Identify the VCE median score column\n",
        "    vce_median_col = None\n",
        "    for col in cleaned_df.columns:\n",
        "        if isinstance(col, str) and 'median' in col.lower() and 'vce' in col.lower():\n",
        "            vce_median_col = col\n",
        "            break\n",
        "        elif isinstance(col, str) and 'median' in col.lower() and 'score' in col.lower():\n",
        "            vce_median_col = col\n",
        "            break\n",
        "    if not vce_median_col:\n",
        "        # Fallback: look for a numeric column with typical VCE score values (20-40)\n",
        "        for col in numeric_cols:\n",
        "            col_mean = cleaned_df[col].mean()\n",
        "            if 20 <= col_mean <= 40:\n",
        "                vce_median_col = col\n",
        "                logging.info(f\"Identified potential VCE median score column: {col} (mean: {col_mean})\")\n",
        "                break\n",
        "    if vce_median_col:\n",
        "        logging.info(f\"Using {vce_median_col} as VCE median score column\")\n",
        "        # Filter out records with a 0 median score\n",
        "        cleaned_df = cleaned_df[cleaned_df[vce_median_col] > 0]\n",
        "\n",
        "    # Identify selective schools and mark their 'Sector' as 'SELECTIVE'\n",
        "    selective_schools = [\n",
        "        'MELBOURNE HIGH SCHOOL',\n",
        "        'MACROBERTSON GIRLS HIGH SCHOOL',\n",
        "        'NOSSAL HIGH SCHOOL',\n",
        "        'SUZANNE CORY HIGH SCHOOL',\n",
        "        'JOHN MONASH SCIENCE SCHOOL'\n",
        "    ]\n",
        "    for school in selective_schools:\n",
        "        for match_type in ['exact', 'contains', 'partial']:\n",
        "            if match_type == 'exact':\n",
        "                matches = cleaned_df[school_col].astype(str).str.upper() == school\n",
        "            elif match_type == 'contains':\n",
        "                matches = cleaned_df[school_col].astype(str).str.upper().str.contains(school, na=False)\n",
        "            else:\n",
        "                key_parts = school.split()\n",
        "                if len(key_parts) >= 2:\n",
        "                    pattern = '|'.join(key_parts)\n",
        "                    matches = cleaned_df[school_col].astype(str).str.upper().str.contains(pattern, na=False)\n",
        "                else:\n",
        "                    continue\n",
        "            if matches.any():\n",
        "                logging.info(f\"Found selective school ({match_type} match): {school}\")\n",
        "                cleaned_df.loc[matches, 'Sector'] = 'SELECTIVE'\n",
        "                break\n",
        "\n",
        "    # Additional pattern matching for selective schools\n",
        "    specific_patterns = {\n",
        "        'Melbourne High': 'MELBOURNE HIGH',\n",
        "        'MacRobertson': 'MACROBERTSON',\n",
        "        'Mac.?Robertson': 'MACROBERTSON',\n",
        "        'Nossal High': 'NOSSAL',\n",
        "        'Suzanne Cory': 'SUZANNE CORY',\n",
        "        'John Monash': 'JOHN MONASH'\n",
        "    }\n",
        "    for name, pattern in specific_patterns.items():\n",
        "        matches = cleaned_df[school_col].astype(str).str.contains(pattern, case=False, na=False, regex=True)\n",
        "        if matches.any():\n",
        "            logging.info(f\"Found selective school using pattern: {pattern}\")\n",
        "            cleaned_df.loc[matches, 'Sector'] = 'SELECTIVE'\n",
        "\n",
        "    # Fallback: Manual keyword check for selective schools\n",
        "    school_keywords = {\n",
        "        'MELBOURNE HIGH': ['MELBOURNE', 'HIGH'],\n",
        "        'MACROBERTSON': ['MACROBERTSON', 'MAC ROBERTSON', 'GIRLS'],\n",
        "        'NOSSAL': ['NOSSAL'],\n",
        "        'SUZANNE CORY': ['SUZANNE', 'CORY'],\n",
        "        'JOHN MONASH': ['JOHN', 'MONASH', 'SCIENCE']\n",
        "    }\n",
        "    for idx, row in cleaned_df.iterrows():\n",
        "        school_name = str(row[school_col]).upper()\n",
        "        for school_key, keywords in school_keywords.items():\n",
        "            if any(keyword in school_name for keyword in keywords):\n",
        "                logging.info(f\"Found potential selective school by keyword: {school_name}\")\n",
        "                cleaned_df.loc[idx, 'Sector'] = 'SELECTIVE'\n",
        "\n",
        "    # Identify high scores percentage column\n",
        "    high_scores_col = None\n",
        "    for col in cleaned_df.columns:\n",
        "        if isinstance(col, str) and '40' in col and ('over' in col.lower() or 'plus' in col.lower()):\n",
        "            high_scores_col = col\n",
        "            break\n",
        "    if not high_scores_col:\n",
        "        for col in numeric_cols:\n",
        "            if col != vce_median_col and col != 'Sector' and col != school_col:\n",
        "                col_values = cleaned_df[col].dropna()\n",
        "                if len(col_values) > 0 and col_values.min() >= 0 and col_values.max() <= 100:\n",
        "                    if col_values.mean() < 30:\n",
        "                        high_scores_col = col\n",
        "                        logging.info(f\"Identified potential high scores percentage column: {col} (mean: {col_values.mean()})\")\n",
        "                        break\n",
        "    if high_scores_col:\n",
        "        logging.info(f\"Using {high_scores_col} as high scores percentage column\")\n",
        "\n",
        "    return cleaned_df, school_col, vce_median_col, high_scores_col\n",
        "\n",
        "# To test this cell, after loading a DataFrame with read_data(), run:\n",
        "df = read_data(\"/content/data/raw/2024SeniorSecondaryCompletionAndAchievementInformation.xlsx\")\n",
        "cleaned_df, school_col, vce_median_col, high_scores_col = clean_data(df)\n",
        "print(\"Cleaned DataFrame head:\\n\", cleaned_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16JvXGS9RKkB",
        "outputId": "2229ec74-1160-4f5d-f03e-37c5df78af3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned DataFrame head:\n",
            "    Senior Secondary Completion and Achievement Information, 2024  Unnamed: 1  \\\n",
            "10                         Academy of Mary Immaculate                      0   \n",
            "14                                     Aitken College                      0   \n",
            "15                                    Al Iman College                      0   \n",
            "16                                  Al Siraat College                      0   \n",
            "17                                   Al-Taqwa College                      0   \n",
            "\n",
            "      Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5 Unnamed: 6 Unnamed: 7  \\\n",
            "10       FITZROY        34.0         8.0         2.0        nan        nan   \n",
            "14     GREENVALE        41.0        14.0         0.0          Y        nan   \n",
            "15  MELTON SOUTH        17.0         0.0         0.0        nan        nan   \n",
            "16        EPPING        20.0         7.0         2.0        nan        nan   \n",
            "17     TRUGANINA        32.0        12.0         0.0          Y        nan   \n",
            "\n",
            "   Unnamed: 8 Unnamed: 9  ...  Unnamed: 14  Unnamed: 15  Unnamed: 16  \\\n",
            "10        nan        nan  ...         93.2        100.0          0.0   \n",
            "14        nan        nan  ...         91.2         99.0          8.0   \n",
            "15        nan        nan  ...         71.4        100.0          0.0   \n",
            "16        nan        nan  ...         97.9         98.0          0.0   \n",
            "17        nan        nan  ...         94.9        100.0         10.0   \n",
            "\n",
            "    Unnamed: 17  Unnamed: 18  Unnamed: 19  Unnamed: 20  Unnamed: 21  \\\n",
            "10         10.0        100.0          0.0         32.0         11.3   \n",
            "14          0.0         99.0          0.0         28.0          4.6   \n",
            "15          0.0          0.0          0.0         26.0          0.8   \n",
            "16          0.0         99.0          0.0         31.0          6.8   \n",
            "17          0.0         76.0          0.0         34.0          9.7   \n",
            "\n",
            "    Unnamed: 22   Sector  \n",
            "10          0.0  REGULAR  \n",
            "14          0.0  REGULAR  \n",
            "15          0.0  REGULAR  \n",
            "16          0.0  REGULAR  \n",
            "17          0.0  REGULAR  \n",
            "\n",
            "[5 rows x 24 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-b4ca2eeaec56>:49: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  cleaned_df[small_school_col] = cleaned_df[small_school_col].replace('*', 1)\n",
            "<ipython-input-53-b4ca2eeaec56>:57: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  cleaned_df = cleaned_df.replace('-', 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 4: Data Analysis\n",
        "def analyze_data_custom(df, school_col, vce_median_col, high_scores_col=None):\n",
        "    \"\"\"\n",
        "    Perform data analysis:\n",
        "      - Log total number of schools.\n",
        "      - Log counts by Sector.\n",
        "      - Compute and log statistics for VCE median scores by Sector.\n",
        "      - If available, compute and log statistics for high scores percentage by Sector.\n",
        "      - List and log the top 20 schools by VCE median score.\n",
        "\n",
        "    Returns:\n",
        "      sector_stats: DataFrame with VCE median score stats by Sector.\n",
        "      sector_high_stats: DataFrame with high scores percentage stats by Sector (if available).\n",
        "      top_schools: DataFrame of the top 20 schools.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Total schools after cleaning: {len(df)}\")\n",
        "\n",
        "    sector_counts = df['Sector'].value_counts()\n",
        "    logging.info(f\"Schools by sector:\\n{sector_counts}\")\n",
        "\n",
        "    sector_stats = df.groupby('Sector')[vce_median_col].agg(['count', 'mean', 'median', 'std'])\n",
        "    logging.info(f\"VCE Median Score by Sector:\\n{sector_stats}\")\n",
        "\n",
        "    if high_scores_col:\n",
        "        sector_high_stats = df.groupby('Sector')[high_scores_col].agg(['count', 'mean', 'median', 'std'])\n",
        "        logging.info(f\"High Scores Percentage by Sector:\\n{sector_high_stats}\")\n",
        "    else:\n",
        "        sector_high_stats = None\n",
        "\n",
        "    top_schools = df.sort_values(vce_median_col, ascending=False).head(20)\n",
        "    if high_scores_col:\n",
        "        logging.info(f\"Top 20 schools by VCE Median Score:\\n{top_schools[[school_col, vce_median_col, high_scores_col, 'Sector']]}\")\n",
        "    else:\n",
        "        logging.info(f\"Top 20 schools by VCE Median Score:\\n{top_schools[[school_col, vce_median_col, 'Sector']]}\")\n",
        "\n",
        "    return sector_stats, sector_high_stats, top_schools\n",
        "\n",
        "# To test this cell, run analysis on the cleaned DataFrame:\n",
        "stats, high_stats, top20 = analyze_data_custom(cleaned_df, school_col, vce_median_col, high_scores_col)\n",
        "print(\"Sector Stats:\\n\", stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17-RnKxIo5Kq",
        "outputId": "ed9fc3e5-5acc-49b8-cdba-6a8a8d181a0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sector Stats:\n",
            "            count       mean  median       std\n",
            "Sector                                       \n",
            "REGULAR      406  28.261084    28.0  3.260813\n",
            "SELECTIVE    109  30.807339    31.0  3.568061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 5: Data Visualization\n",
        "def create_visualizations_custom(df, school_col, vce_median_col, high_scores_col=None):\n",
        "    \"\"\"\n",
        "    Create visualizations:\n",
        "      1. Boxplot: Distribution of VCE median scores by Sector.\n",
        "      2. If available, boxplot: Distribution of high scores percentage by Sector.\n",
        "      3. If available, scatter plot: Correlation between VCE median and high scores percentage.\n",
        "      4. Bar plot: Top 20 schools by VCE median score.\n",
        "      5. Histogram: Distribution of VCE median scores.\n",
        "    \"\"\"\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # 1. Boxplot for VCE median scores by Sector\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.boxplot(x='Sector', y=vce_median_col, data=df)\n",
        "    plt.title('Distribution of VCE Median Scores by Sector')\n",
        "    plt.savefig('visualizations/vce_scores_by_sector.png')\n",
        "    plt.close()\n",
        "\n",
        "    if high_scores_col:\n",
        "        # 2. Boxplot for high scores percentage by Sector\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.boxplot(x='Sector', y=high_scores_col, data=df)\n",
        "        plt.title('Distribution of VCE High Scores (40+) by Sector')\n",
        "        plt.savefig('visualizations/high_scores_by_sector.png')\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Scatter plot for correlation between VCE median and high scores percentage\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        sns.scatterplot(x=vce_median_col, y=high_scores_col, hue='Sector', data=df)\n",
        "        plt.title('Correlation: VCE Median Score vs. High Scores Percentage')\n",
        "        plt.savefig('visualizations/score_correlation.png')\n",
        "        plt.close()\n",
        "\n",
        "    # 4. Bar plot of top 20 schools by VCE median score\n",
        "    top_schools = df.sort_values(vce_median_col, ascending=False).head(20)\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    colors = ['red' if sector == 'SELECTIVE' else 'blue' for sector in top_schools['Sector']]\n",
        "    plt.barh(top_schools[school_col], top_schools[vce_median_col], color=colors)\n",
        "    plt.xlabel('VCE Median Score')\n",
        "    plt.ylabel('School')\n",
        "    plt.title('Top 20 Schools by VCE Median Score')\n",
        "    from matplotlib.patches import Patch\n",
        "    legend_elements = [Patch(facecolor='red', label='SELECTIVE'), Patch(facecolor='blue', label='REGULAR')]\n",
        "    plt.legend(handles=legend_elements)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualizations/top_schools.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Histogram of VCE median scores\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(data=df, x=vce_median_col, bins=20, kde=True)\n",
        "    plt.title('Histogram of VCE Median Scores')\n",
        "    plt.xlabel('VCE Median Score')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.savefig('visualizations/vce_median_histogram.png')\n",
        "    plt.close()\n",
        "\n",
        "    logging.info(\"Visualizations created and saved in the 'visualizations' directory.\")\n",
        "\n",
        "\n",
        "create_visualizations_custom(cleaned_df, school_col, vce_median_col, high_scores_col)\n"
      ],
      "metadata": {
        "id": "iwhWhjGppFCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cell 6: Run the Pipeline in Colab and Fix Final Headers\n",
        "# In Google Colab, use the file uploader to upload your Excel file.\n",
        "try:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()  # Prompts for file upload\n",
        "    uploaded_file_name = list(uploaded.keys())[0]\n",
        "    # Save the uploaded file to the data/raw directory\n",
        "    with open(os.path.join('data/raw', uploaded_file_name), 'wb') as f:\n",
        "        f.write(uploaded[uploaded_file_name])\n",
        "    file_name = uploaded_file_name\n",
        "    logging.info(f\"Uploaded file: {file_name}\")\n",
        "except ImportError:\n",
        "    # If not in Colab, use a local file name\n",
        "    file_name = 'example.xlsx'\n",
        "    logging.info(f\"Using local file: {file_name}\")\n",
        "\n",
        "def main_pipeline(file_name):\n",
        "    \"\"\"\n",
        "    Main pipeline to run the complete process:\n",
        "      - Read the Excel data.\n",
        "      - Clean the data.\n",
        "      - Analyze the data.\n",
        "      - Create visualizations.\n",
        "      - Fix headers and save the cleaned data.\n",
        "    \"\"\"\n",
        "    df = read_data(file_name)\n",
        "    df_cleaned, school_col, vce_median_col, high_scores_col = clean_data(df)\n",
        "\n",
        "    # Optionally run analysis and visualization:\n",
        "    analyze_data_custom(df_cleaned, school_col, vce_median_col, high_scores_col)\n",
        "    create_visualizations_custom(df_cleaned, school_col, vce_median_col, high_scores_col)\n",
        "\n",
        "    # Fix final headers\n",
        "    desired_headers = [\n",
        "        \"School\",\n",
        "        \"Small School\",\n",
        "        \"Locality\",\n",
        "        \"VCE/VM/VET Programs\",\n",
        "        \"VET Certificates\",\n",
        "        \"HES Enrolments\",\n",
        "        \"VM Enrolments\",\n",
        "        \"VPC Enrolments\",\n",
        "        \"IB Diploma Enrolments\",\n",
        "        \"NHT Enrolments\",\n",
        "        \"Students in VCE/VM/VET\",\n",
        "        \"Students in VET\",\n",
        "        \"Students in SBAT\",\n",
        "        \"Students in HES\",\n",
        "        \"VTAC Applications\",\n",
        "        \"VCE Completions\",\n",
        "        \"VM Awards\",\n",
        "        \"VCE (Bacc) Awards\",\n",
        "        \"VET Competency Completion\",\n",
        "        \"HES Completion\",\n",
        "        \"Median VCE Score\",\n",
        "        \"Study Scores 40+\",\n",
        "        \"VPC Awards\",\n",
        "        \"REGULAR\"\n",
        "    ]\n",
        "    if df_cleaned.shape[1] >= len(desired_headers):\n",
        "        # Take the first N columns and assign the desired headers\n",
        "        df_cleaned = df_cleaned.iloc[:, :len(desired_headers)]\n",
        "        df_cleaned.columns = desired_headers\n",
        "    else:\n",
        "        logging.error(\"Cleaned data does not have enough columns to match desired headers.\")\n",
        "\n",
        "    # Save the cleaned data with the fixed headers\n",
        "    df_cleaned.to_csv('data/processed/cleaned_data.csv', index=False)\n",
        "    logging.info(\"Processing complete. Cleaned data saved to 'data/processed/cleaned_data.csv'.\")\n",
        "\n",
        "# To run the full pipeline, simply execute:\n",
        "if __name__ == \"__main__\":\n",
        "    main_pipeline(file_name)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "4pHpM6uepa-B",
        "outputId": "5a4713f7-c0cc-4fdb-f413-345983f9aa23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b999476e-f6e8-4f75-81a0-ea444c7e265b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b999476e-f6e8-4f75-81a0-ea444c7e265b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2024SeniorSecondaryCompletionAndAchievementInformation.xlsx to 2024SeniorSecondaryCompletionAndAchievementInformation.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-b4ca2eeaec56>:49: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  cleaned_df[small_school_col] = cleaned_df[small_school_col].replace('*', 1)\n",
            "<ipython-input-53-b4ca2eeaec56>:57: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  cleaned_df = cleaned_df.replace('-', 0)\n"
          ]
        }
      ]
    }
  ]
}